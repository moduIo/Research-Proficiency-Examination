\relax 
\providecommand\zref@newlabel[2]{}
\citation{Getoor:2007:ISR:1296231}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{Prince:2012:CVM:2344089}
\citation{Russell:2009:AIM:1671238}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (left) A directed graphical model (Bayesian network) factorizes as $p(X) = p(x_1)p(x_2 | x_1)p(x_3|x_1)$. (right) The undirected equivalent Markov network factorizes as $p(X) = \frac  {1}{Z} \psi _1(x_1, x_2) \psi _2(x_1, x_3)$}}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Probabilistic Graphical Models}{2}}
\citation{Freeman13}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A factor graph over three random variables and four factors which factorizes as $p(X) = \frac  {1}{Z}f_1(x_1)f_2(x_1, x_2)f_3(x_1, x_2)f_4(x_2, x_3)$.}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Belief Propagation}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Markov chain with three observed variables $y_1, y_2, y_3$ and three hidden state variables $x_1, x_2, x_3$. The arrows show the messages computed for $p(x_1 | Y)$.}}{4}}
\citation{Prince:2012:CVM:2344089}
\citation{Ng}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Expectation Maximization}{5}}
\citation{Richardson:2006:MLN:1113907.1113910}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Expectation Maximization}}{6}}
\citation{Richardson:2006:MLN:1113907.1113910}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ground MLN over $C = \{A, B\}$}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Case Study: Markov Logic Networks}{7}}
\citation{Sato:1997:PLS:1622270.1622348}
\citation{DeRaedt:2007:PPP:1625275.1625673}
\citation{Muggleton96stochasticlogic}
\citation{Sato95astatistical}
\citation{Getoor:2007:ISR:1296231}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces ProbLog program for the alarm world.}}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Probabilistic Logic Programming}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Distribution Semantics}{8}}
\citation{DeRaedt:2015:PPC:2812521.2812529}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces PRISM program for the alarm world.}}{9}}
\citation{DeRaedt:2015:PPC:2812521.2812529}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Inference}{10}}
\citation{prism}
\citation{Kersting06logicalhidden}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Learning}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Case Study: LOHMM Parameter Fitting in PRISM}{11}}
\citation{Kersting06logicalhidden}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A two layer neural network.}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Deep Learning}{12}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Stochastic Gradient Descent}}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}DNN Architectures}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An autoencoder network.}}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Case Study: Word Embeddings}{15}}
\citation{Mikolov:2013:DRW:2999792.2999959}
\citation{Pennington14glove:global}
\citation{Hu2016HarnessingDN}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deep Neural Networks with Logic Rules}{16}}
\citation{2015arXiv150504406B}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The augmented DNN is a teacher and student network optimizing a dual objective.}}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Logic Rules}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Teacher Network Construction}{18}}
\citation{Kim2014ConvolutionalNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Learning Algorithm}{19}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Teacher and Student Network Training}}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Case Study: Sentence Level Sentiment Analysis}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces CNN architecture using Word2Vec vectors.}}{20}}
\citation{DBLP:journals/corr/Cohen16b}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Results of the teacher and student architecture extending a base CNN model.}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {6}TensorLog}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Knowledge Base Representation}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example $\mathcal  {DB}$ and $\mathcal  {T}$.}}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Factor Graph Construction}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Factor graph representation of clauses.}}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Inference}{24}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces TensorLog Belief Propagation}}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Learning}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{25}}
\bibstyle{unsrt}
\bibdata{rpe}
\bibcite{Getoor:2007:ISR:1296231}{1}
\bibcite{Prince:2012:CVM:2344089}{2}
\bibcite{Russell:2009:AIM:1671238}{3}
\bibcite{Freeman13}{4}
\bibcite{Ng}{5}
\bibcite{Richardson:2006:MLN:1113907.1113910}{6}
\bibcite{Sato:1997:PLS:1622270.1622348}{7}
\bibcite{DeRaedt:2007:PPP:1625275.1625673}{8}
\bibcite{Muggleton96stochasticlogic}{9}
\bibcite{Sato95astatistical}{10}
\bibcite{DeRaedt:2015:PPC:2812521.2812529}{11}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Conclusion}{27}}
\bibcite{prism}{12}
\bibcite{Kersting06logicalhidden}{13}
\bibcite{Mikolov:2013:DRW:2999792.2999959}{14}
\bibcite{Pennington14glove:global}{15}
\bibcite{Hu2016HarnessingDN}{16}
\bibcite{2015arXiv150504406B}{17}
\bibcite{Kim2014ConvolutionalNN}{18}
\bibcite{DBLP:journals/corr/Cohen16b}{19}
